{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31062,"status":"ok","timestamp":1660371704532,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"},"user_tz":-480},"id":"dsWc6in9PgBt","outputId":"f9f25aa9-082f-49bf-ceb6-d03efe6c28c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive')\n","path = \"/content/drive/MyDrive/Colab Notebooks\"\n","os.chdir(path)"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2908,"status":"ok","timestamp":1660371707438,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"},"user_tz":-480},"id":"-9MKVVgtQW08"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from tensorflow import keras\n","from tensorflow.keras import layers,regularizers\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import re\n","import math\n","import json\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import jieba\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from sklearn.metrics import accuracy_score,f1_score\n","from tensorflow.keras.layers import concatenate\n","import numpy as np\n","import random"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1966,"status":"ok","timestamp":1660371709396,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"},"user_tz":-480},"id":"VXsihwC_bEbY"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.utils.data as Data\n","import matplotlib.pyplot as plt\n","\n","torch.set_default_tensor_type(torch.DoubleTensor)\n","\n","SEED = 30\n","\n","torch.manual_seed(SEED) \n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True \n","\n","embedding_size = 300\n","sequence_length = 50\n","num_classes = 2"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":891,"status":"ok","timestamp":1660371710284,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"},"user_tz":-480},"id":"AlwwJMxwQbRZ"},"outputs":[],"source":["stopwords=pd.read_csv(\"./data/CNENstopwords.txt\", index_col=False,sep='\\t',quoting=3,names=['stopwords'],encoding='utf-8')\n","stopwords = stopwords.stopwords.values.tolist()\n","blackword = ['\\t','\\n',' ','\\xa0','\\u200b']\n","for i in blackword:\n","    stopwords.append(i)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1660371710284,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"},"user_tz":-480},"id":"nfwM4oluQcEL"},"outputs":[],"source":["def drop_stopwords(line, stopwords):\n","    line_clean = []\n","    for word in line:\n","        if word in stopwords:\n","            continue\n","        line_clean.append(word)\n","    return line_clean\n","\n","def cleantext(line):\n","    line = line.strip()\n","    line = re.sub(r'[^\\w\\s]','',line)\n","    current_seg = jieba.lcut(line)\n","    for i in range(current_seg.count(' ')):\n","        current_seg.remove(' ')\n","    for i in current_seg:\n","        if i == '\\xa0':\n","            current_seg.remove(i)\n","    current_seg = drop_stopwords(current_seg, stopwords)\n","    return current_seg\n","\n","def cleanurl(line):\n","    line = delgoogleweblight(line)\n","    blackurlparts = ['www', 'com', 'cn', 'http', 'https']\n","    comp = []\n","    for i in list(filter(bool, re.split('[^a-zA-Z]',line))):\n","        if i not in blackurlparts and len(i)>2:\n","            comp.append(i)\n","    return comp\n","\n","def delgoogleweblight(url):\n","    a = url.strip().split('/')\n","    if a[2]=='googleweblight.com':\n","        a.remove('')\n","        a.remove('googleweblight.com')\n","        a.remove('fp?u='+a[0])\n","        return '/'.join(a)\n","    else:\n","        return url"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":580},"executionInfo":{"elapsed":245161,"status":"ok","timestamp":1660371955444,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"},"user_tz":-480},"id":"tSmkGSU9Qe1o","outputId":"46aa36f6-8f19-4943-9b6c-7895e45293ba"},"outputs":[{"output_type":"stream","name":"stderr","text":["Building prefix dict from the default dictionary ...\n","DEBUG:jieba:Building prefix dict from the default dictionary ...\n"]},{"output_type":"stream","name":"stdout","text":["9261 10901\n"]},{"output_type":"stream","name":"stderr","text":["Dumping model to file cache /tmp/jieba.cache\n","DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n","Loading model cost 0.989 seconds.\n","DEBUG:jieba:Loading model cost 0.989 seconds.\n","Prefix dict has been built successfully.\n","DEBUG:jieba:Prefix dict has been built successfully.\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                    text  \\\n","0      [SuperScript, SQLSSRS, MSDN2013, SQL, solution...   \n","1      [Face, recognition, unconstrained, videos, mat...   \n","2      [Weak, Homogenization, Point, Processes, Space...   \n","3      [PDF, Seawater, pH, reconstruction, boron, iso...   \n","4      [000114420416136443txt, SECgovSee, ldquoSupple...   \n","...                                                  ...   \n","27411  [Results, Salton, Seismic, Imaging, Project, S...   \n","27412  [PDF, UTIGTR, 0083pdf, Institute, Geophysicsma...   \n","27413  [数据, 集, DataEase, 文档, 数据库, 数据, 集从, 数据源, 中, 选择,...   \n","27414  [数据, 集, Azure, Data, Factory, Azure, Synapse, ...   \n","27415  [机, 数据库, 查询, 数据库, 导入, 数据, Microsoft, Docs20211...   \n","\n","                                                     url  label  \n","0      [social, msdn, microsoft, Forums, superscript,...      0  \n","1      [researchgate, net, publication, Face, recogni...      0  \n","2      [researchgate, net, publication, Weak, Homogen...      0  \n","3      [researchgate, net, publication, Seawater, rec...      0  \n","4                 [sec, gov, Archives, edgar, data, txt]      0  \n","...                                                  ...    ...  \n","27411  [iris, edu, webinar, results, from, the, salto...      1  \n","27412  [udc, utexas, edu, external, facilities, tech,...      0  \n","27413  [dataease, docs, user, manual, dataset, config...      0  \n","27414  [docs, microsoft, azure, data, factory, concep...      0  \n","27415  [docs, microsoft, power, query, native, databa...      0  \n","\n","[27416 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-bf4835ae-5050-4656-bce2-5b52ec97bd16\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>url</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[SuperScript, SQLSSRS, MSDN2013, SQL, solution...</td>\n","      <td>[social, msdn, microsoft, Forums, superscript,...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[Face, recognition, unconstrained, videos, mat...</td>\n","      <td>[researchgate, net, publication, Face, recogni...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[Weak, Homogenization, Point, Processes, Space...</td>\n","      <td>[researchgate, net, publication, Weak, Homogen...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[PDF, Seawater, pH, reconstruction, boron, iso...</td>\n","      <td>[researchgate, net, publication, Seawater, rec...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[000114420416136443txt, SECgovSee, ldquoSupple...</td>\n","      <td>[sec, gov, Archives, edgar, data, txt]</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>27411</th>\n","      <td>[Results, Salton, Seismic, Imaging, Project, S...</td>\n","      <td>[iris, edu, webinar, results, from, the, salto...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>27412</th>\n","      <td>[PDF, UTIGTR, 0083pdf, Institute, Geophysicsma...</td>\n","      <td>[udc, utexas, edu, external, facilities, tech,...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27413</th>\n","      <td>[数据, 集, DataEase, 文档, 数据库, 数据, 集从, 数据源, 中, 选择,...</td>\n","      <td>[dataease, docs, user, manual, dataset, config...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27414</th>\n","      <td>[数据, 集, Azure, Data, Factory, Azure, Synapse, ...</td>\n","      <td>[docs, microsoft, azure, data, factory, concep...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27415</th>\n","      <td>[机, 数据库, 查询, 数据库, 导入, 数据, Microsoft, Docs20211...</td>\n","      <td>[docs, microsoft, power, query, native, databa...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>27416 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf4835ae-5050-4656-bce2-5b52ec97bd16')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-bf4835ae-5050-4656-bce2-5b52ec97bd16 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-bf4835ae-5050-4656-bce2-5b52ec97bd16');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}],"source":["df_white = pd.read_excel(\"./data/whitedn.xlsx\",usecols=['dn'])\n","df_black = pd.read_excel(\"./data/blackdn.xlsx\",usecols=['dn'])\n","whitednlist = df_white['dn'].values.tolist()\n","blackdnlist = df_black['dn'].values.tolist()\n","with open(\"./data/google_search/google_search_py_yj.json\",'rb') as load_f:\n","     load_dict = json.load(load_f)\n","datayj = load_dict['RECORDS']\n","with open(\"./data/google_search/google_search_py_cj.json\",'rb') as load_f:\n","     load_dict = json.load(load_f)\n","datacj = load_dict['RECORDS']\n","with open(\"./data/google_search/google_search_py_dm.json\",'rb') as load_f:\n","     load_dict = json.load(load_f)\n","datadm = load_dict['RECORDS']\n","data = datayj + datacj + datadm\n","\n","whiteurllist,blackurllist = [],[]\n","blackword=['.pdf','/paper','/article','baidu','bilibili','sohu','weibo','sina','youtube','google','taobao','douban','youdao','baike','wiki','pedia','researchgate',\n","             'amazon','twitter','qq','tecent','patent','worldwidescience','zhuanli','163','rhhz','d-nb','zhihu','zhidao','csdn','jianshu',\n","             'baijiahao','zhuanlan','blog','statesurveys','bartoc','jstor','Cited by','被引用','数据库',' OR ','…']\n","for i in data:\n","    matchObj = i['url_path'].split(' › ')\n","    if matchObj[0] in whitednlist:\n","        cnt = 0\n","        for j in blackword:\n","            if re.search(j, i['url'], re.I) is None:\n","                cnt = cnt + 1\n","        if cnt == len(blackword):\n","            whiteurllist.append(i['url'])\n","        else:\n","            blackurllist.append(i['url'])\n","    if matchObj[0] in blackdnlist:\n","        blackurllist.append(i['url'])\n","whiteurllist=list(set(whiteurllist))\n","blackurllist=list(set(blackurllist))\n","print(len(whiteurllist),len(blackurllist))\n","\n","datadic_test = {'text':[], 'url':[], 'label':[]}\n","for i in data:\n","    if i['url'] in whiteurllist:\n","        datadic_test['text'].append(cleantext(i['title']+i['desc'].strip()))\n","        datadic_test['url'].append(cleanurl(i['url']))\n","        datadic_test['label'].append(1)\n","    elif i['url'] in blackurllist:\n","        datadic_test['text'].append(cleantext(i['title']+i['desc'].strip()))\n","        datadic_test['url'].append(cleanurl(i['url']))\n","        datadic_test['label'].append(0)\n","data_test = pd.DataFrame(datadic_test)\n","data_test"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1660371955444,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"},"user_tz":-480},"id":"rjnkrafDR3dg"},"outputs":[],"source":["def split_train_test_valid(df, train_rate, test_rate):\n","    random.seed(10)\n","    train_len = int(len(df) * train_rate)\n","    test_len = int(len(df) * test_rate)\n","\n","    # split the dataframe\n","    idx = list(df.index)\n","    random.shuffle(idx)  # 将index列表打乱\n","    df_train = df.loc[idx[:train_len]]\n","    df_test = df.loc[idx[train_len:train_len+test_len]]\n","    df_valid = df.loc[idx[train_len+test_len:]]  # 剩下的就是valid\n","    \n","    text_train = df_train['text'].values.tolist()\n","    url_train = df_train['url'].values.tolist()\n","    y_train = df_train['label'].values.tolist()\n","    text_test = df_test['text'].values.tolist()\n","    url_test = df_test['url'].values.tolist()\n","    y_test = df_test['label'].values.tolist()\n","    text_valid = df_valid['text'].values.tolist()\n","    url_valid = df_valid['url'].values.tolist()\n","    y_valid = df_valid['label'].values.tolist()\n","    \n","    return text_train, url_train, y_train, text_test, url_test, y_test, text_valid ,url_valid, y_valid"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1660371955445,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"},"user_tz":-480},"id":"qWI1FifRXawr"},"outputs":[],"source":["text_train, url_train, y_train, text_test, url_test, y_test, text_valid ,url_valid, y_valid = split_train_test_valid(data_test, 0.6, 0.2)\n","origin_train_size = 7000\n","text_train_origin = text_train[:origin_train_size]\n","url_train_origin = url_train[:origin_train_size]\n","y_train_origin = y_train[:origin_train_size]\n","# print(y_train[0])\n","# f = open(\"./data/google_search/text_train_origin\", \"w\")\n","# for line in text_train_origin:\n","#   for word in line:\n","#     f.write(word)\n","#     f.write(\"|\")\n","#   f.write(\"\\n\")\n","# f.close()\n","# f = open(\"./data/google_search/url_train_origin\", \"w\")\n","# for line in url_train_origin:\n","#   for word in line:\n","#     f.write(word)\n","#     f.write(\"|\")\n","#   f.write(\"\\n\")\n","# f.close()\n","# f = open(\"./data/google_search/y_train_origin\", \"w\")\n","# for line in y_train_origin:\n","#   f.write(str(line))\n","#   f.write(\"\\n\")\n","# f.close()"]},{"cell_type":"markdown","metadata":{"id":"3OsELsFIRPYb"},"source":["文本信息处理"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1640,"status":"ok","timestamp":1660371957081,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"},"user_tz":-480},"id":"Bd9_3jQcXIaM","outputId":"1dc7368d-d953-4b4c-99d5-c591d2a4589f"},"outputs":[{"output_type":"stream","name":"stdout","text":["3000\n","1\n"]}],"source":["text_train = []\n","text_uncertain = []\n","base_train_size = 4000\n","f = open(\"./data/google_search/text_train_origin\", \"r\")\n","for i in range(base_train_size):\n","  text_train_str = f.readline()\n","  text_train_str = text_train_str[:-2]\n","  text_train_line = text_train_str.split(\"|\")\n","  text_train.append(text_train_line)\n","for i in range(base_train_size, origin_train_size):\n","  text_uncertain_str = f.readline()\n","  text_uncertain_str = text_uncertain_str[:-2]\n","  text_uncertain_line = text_uncertain_str.split(\"|\")\n","  text_uncertain.append(text_uncertain_line)\n","f.close()\n","url_train = []\n","url_uncertain = []\n","f = open(\"./data/google_search/url_train_origin\", \"r\")\n","for i in range(base_train_size):\n","  url_train_str = f.readline()\n","  url_train_str = url_train_str[:-2]\n","  url_train_line = url_train_str.split(\"|\")\n","  url_train.append(url_train_line)\n","for i in range(base_train_size, origin_train_size):\n","  url_uncertain_str = f.readline()\n","  url_uncertain_str = url_uncertain_str[:-2]\n","  url_uncertain_line = url_uncertain_str.split(\"|\")\n","  url_uncertain.append(url_uncertain_line)\n","f.close()\n","y_train = []\n","y_uncertain = []\n","f = open(\"./data/google_search/y_train_origin\", \"r\")\n","for i in range(base_train_size):\n","  y_train_str = f.readline()\n","  y_train_line = int(y_train_str)\n","  y_train.append(y_train_line)\n","for i in range(base_train_size, origin_train_size):\n","  y_uncertain_str = f.readline()\n","  y_uncertain_str = y_uncertain_str[:-2]\n","  y_uncertain_line = y_uncertain_str.split(\"|\")\n","  y_uncertain.append(y_uncertain_line)\n","f.close()\n","print(len(y_uncertain))\n","print(y_train[0])"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1660371957081,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"},"user_tz":-480},"id":"jaPNFwzuQ5Gi","outputId":"f8c922c0-b7bd-490a-a318-bc5e8fdc6df5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["19999"]},"metadata":{},"execution_count":10}],"source":["max_num_words = 20000\n","tokenizer=Tokenizer(num_words=max_num_words,lower=True,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~� ',oov_token='<OOV>')\n","tokenizer.fit_on_texts(text_train)\n","vocab_text={word:i+1 for i, word in enumerate(tokenizer.word_index) if i+1<max_num_words}\n","len(vocab_text)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7056,"status":"ok","timestamp":1660371964135,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"},"user_tz":-480},"id":"XBFBpOf0RTW8","outputId":"dd7213af-82ee-40b4-e679-59d582fd4a2f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20000, 300])"]},"metadata":{},"execution_count":11}],"source":["from gensim.models import Word2Vec\n","shortmodel = Word2Vec(sentences=text_train, size=300, window=5, min_count=1, workers=4)\n","embedding_matrix_text = np.zeros((len(vocab_text) + 1, 300))\n","for word, i in vocab_text.items():\n","    try:\n","        embedding_vector = shortmodel.wv[str(word)]\n","        embedding_matrix_text[i] = embedding_vector\n","    except KeyError:\n","        continue\n","embedding_matrix_text = torch.from_numpy(embedding_matrix_text)\n","embedding_matrix_text.shape"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1660371964136,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"},"user_tz":-480},"id":"aOwko0hgcEc2","outputId":"c213b5e7-a333-4228-95c8-21ab388b254f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["6881"]},"metadata":{},"execution_count":12}],"source":["max_num_words = 8000\n","tokenizer=Tokenizer(num_words=max_num_words,lower=True,filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~� ',oov_token='<OOV>')\n","tokenizer.fit_on_texts(url_train)\n","vocab_url={word:i+1 for i, word in enumerate(tokenizer.word_index) if i+1<max_num_words}\n","len(vocab_url)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2148,"status":"ok","timestamp":1660371966272,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"},"user_tz":-480},"id":"VDxLLpTOb_hO","outputId":"3551bf23-f380-4853-ce91-80e148b8a968"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([6882, 300])"]},"metadata":{},"execution_count":13}],"source":["urlmodel = Word2Vec(sentences=url_train, size=300, window=5, min_count=1, workers=4)\n","embedding_matrix_url = np.zeros((len(vocab_url) + 1, 300))\n","for word, i in vocab_url.items():\n","    try:\n","        embedding_vector = urlmodel.wv[str(word)]\n","        embedding_matrix_url[i] = embedding_vector\n","    except KeyError:\n","        continue\n","embedding_matrix_url = torch.from_numpy(embedding_matrix_url)\n","embedding_matrix_url.shape"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1660371966272,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"},"user_tz":-480},"id":"3VjfPrC5dIIO","outputId":"153f118d-ba3f-455b-f564-67b99cc3b07b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([26882, 300])"]},"metadata":{},"execution_count":14}],"source":["embedding_matrix = torch.cat((embedding_matrix_text, embedding_matrix_url), 0)\n","embedding_matrix.shape"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":1050,"status":"ok","timestamp":1660371967320,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"},"user_tz":-480},"id":"vwohcArpeaL2"},"outputs":[],"source":["text_sequence_length, url_sequence_length = 50, 8\n","x_train = np.concatenate([pad_sequences(tokenizer.texts_to_sequences(text_train), maxlen=text_sequence_length), pad_sequences(tokenizer.texts_to_sequences(url_train), maxlen=url_sequence_length)+20000],axis=1)\n","x_test = np.concatenate([pad_sequences(tokenizer.texts_to_sequences(text_test), maxlen=text_sequence_length), pad_sequences(tokenizer.texts_to_sequences(url_test), maxlen=url_sequence_length)+20000],axis=1)\n","x_valid = np.concatenate([pad_sequences(tokenizer.texts_to_sequences(text_valid), maxlen=text_sequence_length), pad_sequences(tokenizer.texts_to_sequences(url_valid), maxlen=url_sequence_length)+20000],axis=1)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1660371967320,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"},"user_tz":-480},"id":"hTv1-ALQiTaH"},"outputs":[],"source":["class TextDataset(Data.Dataset):\n","    \"\"\"LongText2000 dataset.\"\"\"\n","\n","    def __init__(self, data_root, data_label):\n","        self.data = data_root\n","        self.label = data_label\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        data = self.data[index]\n","        labels = self.label[index]\n","        return data, labels"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1660371967320,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"},"user_tz":-480},"id":"ZtZW-2GAjxGl","outputId":"42a49b4a-2e25-44de-fd58-d6cf5a6d6fee"},"outputs":[{"output_type":"stream","name":"stdout","text":["40\n","55\n","55\n"]}],"source":["train_loader = Data.DataLoader(TextDataset(x_train, y_train), batch_size=100, shuffle=True, drop_last=False, num_workers=4, pin_memory=True)\n","validate_loader = Data.DataLoader(TextDataset(x_valid, y_valid), batch_size=100, shuffle=True, drop_last=False, num_workers=4, pin_memory=True)\n","test_loader = Data.DataLoader(TextDataset(x_test, y_test), batch_size=100, shuffle=True, drop_last=False, num_workers=4, pin_memory=True)\n","print(len(train_loader))\n","print(len(validate_loader))\n","print(len(test_loader))"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1660371967321,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"},"user_tz":-480},"id":"IWkrCRkWkUmW"},"outputs":[],"source":["class textCNN(nn.Module):\n","    def __init__(self):\n","        super(textCNN, self).__init__()\n","        Knum = 256 \n","        Ks = [3,4,5]\n","        \n","        self.embed = nn.Embedding(len(vocab_text)+len(vocab_url)+2, embedding_size, _weight=embedding_matrix) \n","        \n","        self.convs = nn.ModuleList([nn.Conv2d(1,Knum,(K,embedding_size)) for K in Ks])\n","        self.dropout = nn.Dropout(0.5)\n","        self.fc = nn.Linear(len(Ks)*Knum,2) \n","        \n","    def forward(self,x):\n","        x = self.embed(x) #(N,W,D)\n","        \n","        x = x.unsqueeze(1) #(N,Ci,W,D)\n","        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs] # len(Ks)*(N,Knum,W)\n","        x = [F.max_pool1d(line,line.size(2)).squeeze(2) for line in x]  # len(Ks)*(N,Knum)\n","        \n","        x = torch.cat(x,1) #(N,Knum*len(Ks))\n","        \n","        x = self.dropout(x)\n","        logit = self.fc(x)\n","        return logit\n","\n","if hasattr(torch.cuda, 'empty_cache'):\n","\t  torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"Dpfrb-cjkdhx","executionInfo":{"status":"ok","timestamp":1660371970738,"user_tz":-480,"elapsed":3420,"user":{"displayName":"Tianshu Huang","userId":"05675780912487672392"}}},"outputs":[],"source":["EPOCH = 20\n","model = textCNN().cuda()\n","loss_function = nn.CrossEntropyLoss().cuda()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","lossdata, trainaccdata, validaccdata = [], [], []\n","best_valid_loss = float('inf')\n"]},{"cell_type":"code","source":["for epoch in range(EPOCH):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    total_len = 0\n","    model.train()\n","    for i, (data, labels) in enumerate(train_loader):\n","        b_x = torch.Tensor(data.double()).long()\n","        b_y = torch.Tensor(labels.double()).long()\n","        #forward\n","        if torch.cuda.is_available():\n","            b_x, b_y = b_x.cuda(), b_y.cuda()\n","        output = model(b_x)\n","        #loss\n","        loss = loss_function(output, b_y)\n","        #backward\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        pred_y = torch.max(output, 1)[1].data.squeeze()\n","        acc = (b_y == pred_y)\n","        acc = acc.cpu().numpy().sum()\n","        accuracy = acc / (b_y.size(0))\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += accuracy\n","        total_len += 1\n","        print(\"iter:\", str(i), \"  loss:\", loss.item(), \"  accuracy:\", accuracy)\n","    \n","    trainloss = epoch_loss/total_len\n","    trainaccuracy = epoch_acc/total_len\n","    lossdata.append(trainloss)\n","    trainaccdata.append(trainaccuracy)\n","    print(\"**train result**   epoch:\"+str(epoch)+\"  loss:\"+str(trainloss)+\"  accuracy:\"+str(trainaccuracy))\n","    \n","    epoch_valid_loss = 0\n","    epoch_valid_acc = 0\n","    total_valid_len = 0\n","    model.eval()\n","    with torch.no_grad():\n","        for i, (data, labels) in enumerate(validate_loader):\n","            b_x = torch.Tensor(data.double()).long()\n","            b_y = torch.Tensor(labels.double()).long()\n","            if torch.cuda.is_available():\n","                b_x, b_y = b_x.cuda(), b_y.cuda()\n","            valid_output = model(b_x)\n","            loss = loss_function(valid_output, b_y)\n","            pred_y = torch.max(valid_output, 1)[1].data.squeeze()\n","            acc = (pred_y == b_y)\n","            acc = acc.cpu().numpy().sum()\n","            accuracy = acc / (b_y.size(0))\n","\n","            epoch_valid_loss = epoch_valid_loss + loss.item()\n","            epoch_valid_acc = epoch_valid_acc + accuracy\n","            total_valid_len += 1\n","    \n","    valid_loss = epoch_valid_loss/total_valid_len\n","    valid_acc = epoch_valid_acc/total_valid_len\n","    validaccdata.append(valid_acc)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'CNN-model-short-0413.pt')\n","\n","    print(\"**validation result**   epoch:\"+str(epoch)+\"  loss:\"+str(valid_loss)+\n","          \"  accuracy:\"+str(epoch_valid_acc/total_valid_len))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2V7rzR1I3Fjm","outputId":"e1a03f19-d392-49dd-e184-03c67b219d06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["iter: 0   loss: 0.6945804686196715   accuracy: 0.48\n","iter: 1   loss: 0.6807671205811475   accuracy: 0.58\n","iter: 2   loss: 0.6845548471426881   accuracy: 0.57\n","iter: 3   loss: 0.6834462518071868   accuracy: 0.53\n","iter: 4   loss: 0.6805103987358385   accuracy: 0.51\n","iter: 5   loss: 0.6783402619773932   accuracy: 0.63\n","iter: 6   loss: 0.6728349617788715   accuracy: 0.67\n","iter: 7   loss: 0.6732721003519111   accuracy: 0.56\n","iter: 8   loss: 0.6447896136525426   accuracy: 0.69\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1X7e5ApjYLbB"},"outputs":[],"source":["plt.plot(trainaccdata)\n","plt.plot(validaccdata)\n","plt.plot(lossdata)\n","plt.legend(['train-accuracy', 'valid-accuracy', 'loss'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SuYeF1T6Wr-r"},"outputs":[],"source":["model.load_state_dict(torch.load('CNN-model-short-0413.pt'))\n","model.eval()\n","\n","test_loss = 0\n","test_acc = 0\n","test_len = 0\n","with torch.no_grad():\n","    for i, (data, labels) in enumerate(test_loader):\n","        test_x = torch.Tensor(data.double()).long()\n","        test_y = torch.Tensor(labels.double()).long()\n","        if torch.cuda.is_available():\n","            test_x, test_y = test_x.cuda(), test_y.cuda()\n","        test_output = model(test_x)\n","        loss = loss_function(test_output, test_y)\n","        pred_y = torch.max(test_output, 1)[1].data.squeeze()\n","        acc = (pred_y == test_y)\n","        acc = acc.cpu().numpy().sum()\n","        accuracy = acc / (test_y.size(0))\n","        \n","        test_loss +=  loss.item()\n","        test_acc += accuracy\n","        test_len += 1\n","\n","print(\"**Test result**  loss:\"+str(test_loss/test_len)+\"  accuracy:\"+str(test_acc/test_len))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1n7saS0IW382"},"outputs":[],"source":["df_annotation = pd.read_excel('./data/标注网站（短文本）.xlsx')\n","url = list(df_annotation['url'].apply(cleanurl))\n","text = list(df_annotation['text'].apply(cleantext))\n","data_test = np.concatenate([pad_sequences(tokenizer.texts_to_sequences(text), maxlen=text_sequence_length), pad_sequences(tokenizer.texts_to_sequences(url), maxlen=url_sequence_length)+20000],axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B8nXRPTOy6_g"},"outputs":[],"source":["model = textCNN().cuda()\n","model.load_state_dict(torch.load('CNN-model-short-0413.pt'))\n","model.eval()\n","with torch.no_grad():\n","  xdata = torch.Tensor(data_test).long().cuda()\n","  test_output = model(xdata)\n","  pred_y = torch.max(test_output, 1)[1].data.squeeze()\n","  np.set_printoptions(suppress=True)\n","  np.set_printoptions(precision=4)\n","  print(test_output.cpu().numpy())\n","  print(pred_y)\n","  for i in pred_y:\n","    print(i.item())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GcjrXU-5a7ce"},"outputs":[],"source":["import torch\n","from torch import nn\n","\n","def normalization(p, n):\n","    unnormalized_pred = torch.tensor([p, n])\n","    normalized_pred = nn.Softmax(dim=0)(unnormalized_pred)\n","    return normalized_pred\n","\n","\n","def least_confidence(p, n):\n","    tensor = normalization(p, n)\n","    p = tensor[0]\n","    q = tensor[1]\n","    if p > q:\n","        return p\n","    else:\n","        return q\n","\n","\n","def entropy_reduction(p, n):\n","    tensor = normalization(p, n)\n","    tensor = tensor * torch.log(tensor)\n","    print(\"tensor = \", tensor)\n","    p = tensor[0]\n","    q = tensor[1]\n","    return p + q\n","    #之后取最小的为最需要标注的，故此处为+\n","\n","\n","def margin_sampling(p, n):\n","    tensor = normalization(p, n)\n","    print(\"tensor = \", tensor)\n","    p = tensor[0]\n","    q = tensor[1]\n","    if p > q:\n","        return p - q\n","    else:\n","        return q - p\n","\n","\n","def uncertainty(p, n, uncertainty):\n","    if uncertainty == \"least_confidence\":\n","      return least_confidence(p, n)\n","    elif uncertainty == \"entropy_reduction\":\n","      return entropy_reduction(p, n)\n","    elif uncertainty == \"margin_sampling\":\n","      return margin_sampling(p, n)\n","    else:\n","      return (p, n)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6HvIR_nQLjEe"},"outputs":[],"source":["data_test = np.concatenate([pad_sequences(tokenizer.texts_to_sequences(text_uncertain), maxlen=text_sequence_length), pad_sequences(tokenizer.texts_to_sequences(url_uncertain), maxlen=url_sequence_length)+20000],axis=1)\n","model = textCNN().cuda()\n","model.load_state_dict(torch.load('CNN-model-short-0413.pt'))\n","model.eval()\n","with torch.no_grad():\n","  xdata = torch.Tensor(data_test).long().cuda()\n","  test_output = model(xdata)\n","  print(test_output.shape)\n","  pred_y = torch.max(test_output, 1)[1].data.squeeze()\n","  np.set_printoptions(suppress=True)\n","  np.set_printoptions(precision=4)\n","  print(test_output.cpu().numpy())\n","  print(pred_y)\n","  print(pred_y.shape)\n","uncertainty_dict = dict()\n","for i in range(origin_train_size - base_train_size):\n","  uncertainty_result = uncertainty(test_output[i][0], test_output[i][1], \"margin_sampling\")\n","  uncertainty_dict[base_train_size + i] = uncertainty_result\n","\n","uncertainty_new = 1000\n","uncertainty_list = sorted(uncertainty_dict.items(), key=lambda x: x[1], reverse=False)\n","uncertainty_list = uncertainty_list[:uncertainty_new]\n","uncertainty_line = []\n","for i in range(uncertainty_new):\n","  uncertainty_line.append(uncertainty_list[i][0])\n","print(uncertainty_line)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5oIh3-BownkD"},"outputs":[],"source":["# f = open(\"./data/google_search/text_train_uncertainty3\", \"w\")\n","# for line_number, line in enumerate(text_train_origin):\n","#   if uncertainty_line.count(line_number):\n","#     for word in line:\n","#       f.write(word)\n","#       f.write(\"|\")\n","#     f.write(\"\\n\")\n","# f.close()\n","# f = open(\"./data/google_search/url_train_uncertainty3\", \"w\")\n","# for line_number, line in enumerate(url_train_origin):\n","#   if uncertainty_line.count(line_number):\n","#     for word in line:\n","#       f.write(word)\n","#       f.write(\"|\")\n","#     f.write(\"\\n\")\n","# f.close()\n","# f = open(\"./data/google_search/y_train_uncertainty3\", \"w\")\n","# for line_number, line in enumerate(y_train_origin):\n","#   if uncertainty_line.count(line_number):\n","#     f.write(str(line))\n","#     f.write(\"\\n\")\n","# f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xVxbuhMcZls_"},"outputs":[],"source":["import random\n","\n","random_line = []\n","random_count = 0;\n","while random_count < 1000:\n","  random_num = random.randint(4000, 6999)\n","  if random_line.count(random_num) == 0:\n","    random_line.append(random_num)\n","    random_count += 1\n","\n","f = open(\"./data/google_search/text_train_random\", \"w\")\n","for line_number, line in enumerate(text_train_origin):\n","  if random_line.count(line_number):\n","    for word in line:\n","      f.write(word)\n","      f.write(\"|\")\n","    f.write(\"\\n\")\n","f.close()\n","f = open(\"./data/google_search/url_train_random\", \"w\")\n","for line_number, line in enumerate(url_train_origin):\n","  if random_line.count(line_number):\n","    for word in line:\n","      f.write(word)\n","      f.write(\"|\")\n","    f.write(\"\\n\")\n","f.close()\n","f = open(\"./data/google_search/y_train_random\", \"w\")\n","for line_number, line in enumerate(y_train_origin):\n","  if random_line.count(line_number):\n","    f.write(str(line))\n","    f.write(\"\\n\")\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dEdPlNbNZmP"},"outputs":[],"source":["text_tune = []\n","f = open(\"./data/google_search/text_train_random\", \"r\")\n","for i in range(uncertainty_new):\n","  text_tune_str = f.readline()\n","  text_tune_str = text_tune_str[:-2]\n","  text_tune_line = text_tune_str.split(\"|\")\n","  text_tune.append(text_tune_line)\n","f.close()\n","url_tune = []\n","f = open(\"./data/google_search/url_train_random\", \"r\")\n","for i in range(uncertainty_new):\n","  url_tune_str = f.readline()\n","  url_tune_str = url_tune_str[:-2]\n","  url_tune_line = url_tune_str.split(\"|\")\n","  url_tune.append(url_tune_line)\n","f.close()\n","y_tune = []\n","f = open(\"./data/google_search/y_train_random\", \"r\")\n","for i in range(uncertainty_new):\n","  y_tune_str = f.readline()\n","  y_tune_line = int(y_tune_str)\n","  y_tune.append(y_tune_line)\n","f.close()\n","print(len(y_tune), len(text_tune), len(url_tune))\n","print(y_tune[0])\n","\n","\n","x_tune = np.concatenate([pad_sequences(tokenizer.texts_to_sequences(text_tune), maxlen=text_sequence_length), pad_sequences(tokenizer.texts_to_sequences(url_tune), maxlen=url_sequence_length)+20000],axis=1)\n","tune_loader = Data.DataLoader(TextDataset(x_tune, y_tune), batch_size=100, shuffle=True, drop_last=False, num_workers=4, pin_memory=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CwYGP_apIPHv"},"outputs":[],"source":["model = textCNN().cuda()\n","model.load_state_dict(torch.load('CNN-model-short-0413.pt'))\n","\n","epoch_tune = 5\n","\n","for name, para in model.named_parameters():\n","  print(name)\n","  if name != 'fc.weight' and name != 'fc.bias':\n","    para.requires_grad=False\n","  print(para.requires_grad)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2owV4jOytjZ"},"outputs":[],"source":["optimizer = optim.Adam(filter(lambda p:p.requires_grad,model.parameters()), lr=1e-3)\n","\n","for epoch in range(epoch_tune):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    total_len = 0\n","    model.train()\n","    for i, (data, labels) in enumerate(tune_loader):\n","        b_x = torch.Tensor(data.double()).long()\n","        b_y = torch.Tensor(labels.double()).long()\n","        #forward\n","        if torch.cuda.is_available():\n","            b_x, b_y = b_x.cuda(), b_y.cuda()\n","        output = model(b_x)\n","        #loss\n","        loss = loss_function(output, b_y)\n","        #backward\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        pred_y = torch.max(output, 1)[1].data.squeeze()\n","        acc = (b_y == pred_y)\n","        acc = acc.cpu().numpy().sum()\n","        accuracy = acc / (b_y.size(0))\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += accuracy\n","        total_len += 1\n","        print(\"iter:\", str(i), \"  loss:\", loss.item(), \"  accuracy:\", accuracy)\n","    \n","    trainloss = epoch_loss/total_len\n","    trainaccuracy = epoch_acc/total_len\n","    lossdata.append(trainloss)\n","    trainaccdata.append(trainaccuracy)\n","    print(\"**train result**   epoch:\"+str(epoch)+\"  loss:\"+str(trainloss)+\"  accuracy:\"+str(trainaccuracy))\n","    \n","    epoch_valid_loss = 0\n","    epoch_valid_acc = 0\n","    total_valid_len = 0\n","    model.eval()\n","    with torch.no_grad():\n","        for i, (data, labels) in enumerate(validate_loader):\n","            b_x = torch.Tensor(data.double()).long()\n","            b_y = torch.Tensor(labels.double()).long()\n","            if torch.cuda.is_available():\n","                b_x, b_y = b_x.cuda(), b_y.cuda()\n","            valid_output = model(b_x)\n","            loss = loss_function(valid_output, b_y)\n","            pred_y = torch.max(valid_output, 1)[1].data.squeeze()\n","            acc = (pred_y == b_y)\n","            acc = acc.cpu().numpy().sum()\n","            accuracy = acc / (b_y.size(0))\n","\n","            epoch_valid_loss = epoch_valid_loss + loss.item()\n","            epoch_valid_acc = epoch_valid_acc + accuracy\n","            total_valid_len += 1\n","    \n","    valid_loss = epoch_valid_loss/total_valid_len\n","    valid_acc = epoch_valid_acc/total_valid_len\n","    validaccdata.append(valid_acc)\n","    \n","\n","    print(\"**validation result**   epoch:\"+str(epoch)+\"  loss:\"+str(valid_loss)+\n","          \"  accuracy:\"+str(epoch_valid_acc/total_valid_len))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wymrq6j51niP"},"outputs":[],"source":["df_annotation = pd.read_excel('./data/标注网站（短文本）.xlsx')\n","url = list(df_annotation['url'].apply(cleanurl))\n","text = list(df_annotation['text'].apply(cleantext))\n","data_test = np.concatenate([pad_sequences(tokenizer.texts_to_sequences(text), maxlen=text_sequence_length), pad_sequences(tokenizer.texts_to_sequences(url), maxlen=url_sequence_length)+20000],axis=1)\n","model = textCNN().cuda()\n","model.load_state_dict(torch.load('CNN-model-short-0413.pt'))\n","model.eval()\n","with torch.no_grad():\n","  xdata = torch.Tensor(data_test).long().cuda()\n","  test_output = model(xdata)\n","  pred_y = torch.max(test_output, 1)[1].data.squeeze()\n","  np.set_printoptions(suppress=True)\n","  np.set_printoptions(precision=4)\n","  print(test_output.cpu().numpy())\n","  print(pred_y)\n","  for i in pred_y:\n","    print(i.item())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6kkSlijBzy-M"},"outputs":[],"source":["df_annotation = pd.read_excel('./data/test_专家标注.xlsx')\n","url = list(df_annotation['url'].apply(cleanurl))\n","text = list(df_annotation['text'].apply(cleantext))\n","data_test = np.concatenate([pad_sequences(tokenizer.texts_to_sequences(text), maxlen=text_sequence_length), pad_sequences(tokenizer.texts_to_sequences(url), maxlen=url_sequence_length)+20000],axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11081,"status":"ok","timestamp":1649902121463,"user":{"displayName":"萧峰","userId":"00087177637005829226"},"user_tz":-480},"id":"Ctnl3OTiz7m0","outputId":"dd258218-66cc-4818-b879-5c8f461ec5ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 0.8572 -1.5965]\n"," [ 3.3726 -4.7692]\n"," [-1.96    1.3772]\n"," [ 0.1851 -0.5574]\n"," [-2.1963  1.4971]\n"," [-0.5852  0.6297]\n"," [-0.4383 -0.3263]\n"," [-2.521   2.0062]\n"," [-0.0932 -0.3486]\n"," [-0.9974  0.8558]\n"," [-1.3429  1.2821]\n"," [-1.3155  0.711 ]\n"," [ 0.4079 -1.4928]\n"," [-3.9044  2.9942]\n"," [-0.3913 -0.0354]\n"," [-1.0962  0.6422]\n"," [-3.1715  2.4153]\n"," [-4.0781  3.2668]\n"," [ 0.2179 -1.0006]\n"," [ 1.707  -3.2022]\n"," [ 2.1403 -2.6483]\n"," [ 2.4852 -3.7152]\n"," [-1.0268 -0.1236]\n"," [ 0.8063 -1.2209]\n"," [-0.9913  0.9059]\n"," [ 3.07   -4.1469]\n"," [-2.0972  0.9282]\n"," [-0.4184  0.1879]\n"," [-0.1747 -0.1564]\n"," [-2.0159  1.9254]\n"," [-0.415   0.0411]\n"," [-0.5729 -0.4185]\n"," [ 1.663  -3.1132]\n"," [ 1.3453 -1.854 ]\n"," [-0.3716 -0.7024]\n"," [ 1.2584 -2.1513]\n"," [ 0.9656 -2.1928]\n"," [ 2.9844 -4.4559]\n"," [-0.5287  0.5289]\n"," [-0.7447  0.8353]\n"," [ 0.9264 -1.5305]\n"," [ 2.3951 -2.2797]\n"," [-1.7441  1.2047]\n"," [ 3.8032 -5.017 ]\n"," [-2.657   2.2112]\n"," [-1.9695  0.9624]\n"," [-3.323   2.9441]\n"," [-0.6805  0.1759]\n"," [ 0.3442 -1.1107]\n"," [ 0.7594 -1.471 ]\n"," [ 0.0581 -1.7733]\n"," [-0.7965  0.118 ]\n"," [-0.6135 -0.2583]\n"," [-0.3251 -0.3949]\n"," [ 0.729  -1.3977]\n"," [-0.7513  0.0231]\n"," [ 1.4983 -2.189 ]\n"," [ 1.4983 -2.189 ]\n"," [ 1.7744 -2.5989]\n"," [ 1.4983 -2.189 ]\n"," [ 0.1198 -0.1546]\n"," [-0.2358 -0.2178]\n"," [ 0.5645 -0.8223]\n"," [-0.0749 -1.1362]\n"," [-2.4856  1.8991]\n"," [ 3.2426 -4.1183]\n"," [-0.4638 -0.4316]\n"," [ 1.2343 -2.3726]\n"," [ 2.8707 -3.4497]\n"," [ 0.8385 -1.0942]\n"," [-0.7122  0.343 ]\n"," [-0.3305 -0.413 ]\n"," [ 1.7071 -2.6454]\n"," [ 1.5579 -2.7496]\n"," [ 0.2738 -0.3472]\n"," [-2.7458  2.3651]\n"," [-0.5526 -0.4685]\n"," [-3.2135  2.3401]\n"," [ 1.9876 -2.7364]\n"," [ 0.0049 -1.9009]\n"," [-1.906   0.3875]\n"," [ 1.3942 -2.5652]\n"," [ 0.4537 -1.139 ]\n"," [-2.6137  2.3044]\n"," [-3.5192  3.5449]\n"," [-1.7417  1.3061]\n"," [-2.2757  1.8532]\n"," [-0.7931  0.2034]\n"," [-0.9195  0.2102]\n"," [-3.3423  2.5946]\n"," [-2.425   1.6154]\n"," [-0.141  -0.5601]\n"," [-2.1824  1.8335]\n"," [-1.8528  0.8438]\n"," [-0.0509 -0.3036]\n"," [-0.6421 -0.3023]\n"," [-1.3226  0.6896]\n"," [-0.3962 -0.2373]\n"," [-1.0708 -0.2943]\n"," [-2.0777  0.4558]\n"," [-1.6662  1.2063]\n"," [-2.47    1.1819]\n"," [-1.8095  0.7698]\n"," [ 1.0158 -2.5629]\n"," [-1.8618  1.238 ]\n"," [-0.9482  0.8218]\n"," [-1.029   0.3532]\n"," [-1.5158  1.3198]\n"," [ 0.0918 -0.0436]\n"," [ 5.0534 -5.9728]\n"," [-0.4335 -0.3257]\n"," [-1.417   1.9309]\n"," [-2.6305  1.9322]\n"," [ 0.5478 -0.9608]\n"," [-0.5916 -0.0366]]\n","tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n","        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n","        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n","        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n","        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1],\n","       device='cuda:0')\n"]}],"source":["model = textCNN().cuda()\n","model.load_state_dict(torch.load('CNN-model-short-0413.pt'))\n","model.eval()\n","with torch.no_grad():\n","  xdata = torch.Tensor(data_test).long().cuda()\n","  test_output = model(xdata)\n","  pred_y = torch.max(test_output, 1)[1].data.squeeze()\n","  np.set_printoptions(suppress=True)\n","  np.set_printoptions(precision=4)\n","  print(test_output.cpu().numpy())\n","  print(pred_y)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"813ShortTextCNN+URL.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}